{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 2 - Deep Convolutional Neural Network - Cats vs Dogs\n",
        "\n",
        "Name : Akhila Kalpuri\n",
        "\n",
        "Date : 03-23-2024"
      ],
      "metadata": {
        "id": "c9F-J8sK0tga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading all the required libraries and functions\n"
      ],
      "metadata": {
        "id": "hqk01v9w04my"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fIxSWJTMzEZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import shutil\n",
        "import pathlib\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the JSON Activation Code"
      ],
      "metadata": {
        "id": "hEECRh0A1gaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "i-cVRel3NEzL",
        "outputId": "196216e8-f9b6-47d3-9126-3ffa3bb85eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-08092fba-a74a-466b-9d34-74dcd6d9a3da\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-08092fba-a74a-466b-9d34-74dcd6d9a3da\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving kaggle.json to kaggle (2).json"
      ],
      "metadata": {
        "id": "ixwcQ5_D2Y7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the Data"
      ],
      "metadata": {
        "id": "gemmYINkekQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12"
      ],
      "metadata": {
        "id": "cYjKBp4i2vjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK1YVZ7qQnxY",
        "outputId": "bc4c6708-f374-4470-900b-aa91f3e47241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping The Data"
      ],
      "metadata": {
        "id": "M8kckhEX22wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o -qq dogs-vs-cats.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTVWDe6lQvAZ",
        "outputId": "5d63a84a-582c-499b-f083-7016d32ab880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open dogs-vs-cats.zip, dogs-vs-cats.zip.zip or dogs-vs-cats.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping Train Data"
      ],
      "metadata": {
        "id": "u-F4KDLO3pfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o -qq train.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLDFCkWjQ7S4",
        "outputId": "726bdc99-4636-414d-cb3d-5ecf48e25a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv98yVM3RRS1",
        "outputId": "a4d9b150-0c31-40dd-a857-ea45489809d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.4.23)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.36.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.43.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.11.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (1.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the Cats & Dogs example. Start initially with a training sample of 1000, a validation sample of 500, and a test sample of 500 (like in the text). Use any technique to reduce overfitting and improve performance in developing a network that you train from scratch. What performance did you achieve?\n",
        "\n"
      ],
      "metadata": {
        "id": "Fw0tXZxO4EEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating directory named cats vs dogs small to store the images into 3 subsets named train, validation and test and Dividing the training sample of 1000, a validation sample of 500, and a test sample of 500"
      ],
      "metadata": {
        "id": "mA0vX5TSezLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "shutil.rmtree(\"cats_vs_dogs_small/train/cat\")\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "for category in (\"cat\", \"dog\"):\n",
        "dir = new_base_dir / subset_name / category\n",
        "os.makedirs(dir, exist_ok = True)\n",
        "fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "for fname in fnames:\n",
        "shutil.copyfile(src=original_dir / fname,\n",
        "dst=dir / fname)\n",
        "make_subset(\"train\", start_index=0, end_index=500)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1250)\n",
        "make_subset(\"test\", start_index=1500, end_index=1750)"
      ],
      "metadata": {
        "id": "RrBFYyuTRXh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-Processing and functions - Using image_dataset_from_directory to read images and functions"
      ],
      "metadata": {
        "id": "SaP4xZ9t4OzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"train\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"validation\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"test\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)"
      ],
      "metadata": {
        "id": "di-vj0LXR3oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 1000 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "E9Z9XrvM4o3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 files belonging to 2 classes.\n"
      ],
      "metadata": {
        "id": "pe36DnjHe8qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 files belonging to 2 classes.\n"
      ],
      "metadata": {
        "id": "SDLfJjlte-1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing the shape of the images."
      ],
      "metadata": {
        "id": "_epgejAtfAcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "print(\"data batch shape:\", data_batch.shape)\n",
        "print(\"labels batch shape:\", labels_batch.shape)\n",
        "break"
      ],
      "metadata": {
        "id": "AjmOlIBcR-V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data batch shape: (32, 180, 180, 3)"
      ],
      "metadata": {
        "id": "N5FlwaT9415h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "labels batch shape: (32,)\n",
        "\n"
      ],
      "metadata": {
        "id": "N0zeFFb0fE2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 1 MaxPooling Operation with Increase in filters from 32 to 256 in 5 Input Layers : Instan- tiating a small convnet for dogs vs. cats classification\n"
      ],
      "metadata": {
        "id": "QEvnhKtDfHeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "#Instantiating a small convnet for dogs vs. cats classification\n",
        "#*Model - 1 MaxPooling Operation with Increase in filters from 32 to 256 in 5␣\n",
        "↪Input Layers*\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "Qk8ASmKBSDWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Model - 1"
      ],
      "metadata": {
        "id": "FOWS-02k5Ib7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "C9fXhqHYSIio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilimg the results of the model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"])\n",
        "# Saving the results of the model\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath= \"model1.keras\",\n",
        "save_best_only= True,\n",
        "monitor= \"val_loss\"\n",
        ")\n",
        "# Fitting/Running the Model\n",
        "Model_1 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 30,\n",
        "validation_data= validation_dataset,\n",
        "callbacks= callbacks)"
      ],
      "metadata": {
        "id": "oqVrvSEdSNhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the visuals of the Training and Validation Accuracy/Loss"
      ],
      "metadata": {
        "id": "TSss59LM5num"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_1.history[\"accuracy\"]\n",
        "val_accuracy = Model_1.history[\"val_accuracy\"]\n",
        "loss = Model_1.history[\"loss\"]\n",
        "val_loss = Model_1.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\",␣\n",
        "↪label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3XpQ0z7lSSK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the performance of Model_1 on test set"
      ],
      "metadata": {
        "id": "gS6o5-uq51Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\"model1.keras\")\n",
        "Model1_Results = test_model.evaluate(test_dataset)\n",
        "print(f'Loss: {Model1_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model1_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "kSne7EKHSZ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Measures to Avoid Overfitting Data Augmentation"
      ],
      "metadata": {
        "id": "FVRe15-x6BjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To deprecate warnings that are making the output look clumsy\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True"
      ],
      "metadata": {
        "id": "O7rVD9Pjffyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using few of the techniques such as random flip, random zoom, random rotation so as to create augmented versions of the image"
      ],
      "metadata": {
        "id": "cZ-Jd2WH6QwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "[\n",
        "layers.RandomFlip(\"horizontal\"),\n",
        "layers.RandomRotation(0.1),\n",
        "layers.RandomZoom(0.2)\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "No-01ojrSe2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the augmented images"
      ],
      "metadata": {
        "id": "juqn2Gp16Z3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "for i in range(9):\n",
        "augmented_images = data_augmentation(images)\n",
        "ax = plt.subplot(3, 3, i + 1)\n",
        "plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "RpJjYTmTSpZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 2 MaxPooling Operation with Increase in filters from 32 to 256 in 5 Input Layers with the data being used from the Augmented Images and a droput rate of 0.5*"
      ],
      "metadata": {
        "id": "iDYA6fH66fxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "0Jm6DMevSrFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model 2"
      ],
      "metadata": {
        "id": "PHXIZiIX6mL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"])\n",
        "# Saving the results of the model\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath= \"model2.keras\",\n",
        "save_best_only= True,\n",
        "monitor= \"val_loss\")\n",
        "# Fitting/Running the Model\n",
        "Model_2 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 30,\n",
        "validation_data= validation_dataset,\n",
        "callbacks= callbacks)"
      ],
      "metadata": {
        "id": "_H3OjMjkS2vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Training and Validation Accuracy/Loss 2"
      ],
      "metadata": {
        "id": "1nYP1NKw6ygt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_2.history[\"accuracy\"]\n",
        "val_accuracy = Model_2.history[\"val_accuracy\"]\n",
        "loss = Model_2.history[\"loss\"]\n",
        "val_loss = Model_2.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\",␣\n",
        "↪label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KeH84EP0S60g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the performance of Model_2 on the test set"
      ],
      "metadata": {
        "id": "knGwusjB67q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\"model2.keras\")\n",
        "Model2_Results = test_model.evaluate(test_dataset)\n",
        "print(f'Loss: {Model2_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model2_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "AzxOWToHS_vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the Model 1 and Model 2 : we can clearly see that the accuracy rate of model 2 is higher than model 1\n",
        "\n"
      ],
      "metadata": {
        "id": "XWOyhFys7Ile"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 3 MaxPooling Operation with Increase in filters from 32 to 512 in 6 Input Layers with the use of Augmented Images and Dropout rate of 0.5"
      ],
      "metadata": {
        "id": "Q4JxvvCLft6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=512, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "pJZLG1VpTIBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model summary"
      ],
      "metadata": {
        "id": "0Hxp5rGY7TFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "2kqVnvSJTT49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model 3"
      ],
      "metadata": {
        "id": "iTyU8T1c7dlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model.compile(loss= \"binary_crossentropy\",\n",
        "optimizer= \"adam\",\n",
        "metrics= [\"accuracy\"])\n",
        "# Monitoring the best validation loss using Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath= \"model3.keras\",\n",
        "save_best_only= True,\n",
        "monitor= \"val_loss\")\n",
        "# Model Fit\n",
        "Model_3 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 30,\n",
        "validation_data= validation_dataset,\n",
        "callbacks= callbacks)"
      ],
      "metadata": {
        "id": "GBzX9Z8_TUnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Training and Validation Accuracy/Loss 2"
      ],
      "metadata": {
        "id": "A9TqpN_w7pW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_3.history[\"accuracy\"]\n",
        "val_accuracy = Model_3.history[\"val_accuracy\"]\n",
        "loss = Model_3.history[\"loss\"]\n",
        "val_loss = Model_3.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\", linestyle=\"dashed\",␣\n",
        "↪label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ivNJR9nYTczG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the performance of Model_2 on the test set"
      ],
      "metadata": {
        "id": "263Jgo0170LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model3.keras\")\n",
        "Model3_Results = best_model.evaluate(test_dataset)\n",
        "print(f'Loss: {Model3_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model3_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "dtnlWt-dTnDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 4 MaxPooling Operation with Increase in filters from 64 to 1024 in 5 Input Layers with the use of Augmented Images and Dropout rate of 0.6"
      ],
      "metadata": {
        "id": "kszH_41K77SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180,180,3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=512, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=1024, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.6)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "hXkpPs8iTrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model summary"
      ],
      "metadata": {
        "id": "mJw3nphm8IkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "_2aSIrJITySK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model.compile(loss= \"binary_crossentropy\",\n",
        "optimizer= \"adam\",\n",
        "metrics= ['accuracy'])\n",
        "# Monitoring the best validation loss using Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath = \"model4.keras\",\n",
        "save_best_only= True,\n",
        "monitor= \"val_loss\"\n",
        ")\n",
        "# Model Fit\n",
        "Model_4 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 30,\n",
        "validation_data= validation_dataset,\n",
        "callbacks= callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "CCdl6sWdT2cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Training and Validation Accuracy/Loss 2"
      ],
      "metadata": {
        "id": "9jl4fZdh8nq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_4.history[\"accuracy\"]\n",
        "val_accuracy = Model_4.history[\"val_accuracy\"]\n",
        "loss = Model_4.history[\"loss\"]\n",
        "val_loss = Model_4.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\", linestyle=\"dashed\",␣\n",
        "↪label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yvinrk7sT8Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the performance of Model_2 on the test set"
      ],
      "metadata": {
        "id": "_SMQiMp_8zeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model4.keras\")\n",
        "Model4_Results = best_model.evaluate(test_dataset)\n",
        "print(f'Loss: {Model4_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model4_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "zc5wdAsPUAi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary for Question 1: We did attempt to construct four models using a thousand as the training sample. Let's now evaluate the accuracy and loss of all four models to determine which produces the best results.\n",
        "\n"
      ],
      "metadata": {
        "id": "t2cZ7ZTr868T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: filters from 32 to 256, 5 Input Layers\n"
      ],
      "metadata": {
        "id": "yCPuGvKr9D4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5 Model 3: filters from 32 to 512, 6 Input Layers, Augmented Images and Dropout rate of 0.5 Model 4: filters from 64 to 1024, 5 Input Layers, Augmented Images and Dropout rate of 0.6"
      ],
      "metadata": {
        "id": "CNBG-m50gUEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_1 = (0.653, 0.696)\n",
        "Model_2 = (0.588, 0.714)\n",
        "Model_3 = (0.620,0.674)\n",
        "Model_4 = (0.604, 0.680)"
      ],
      "metadata": {
        "id": "s3WpZJ7qUIeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Models = (\"Mod_1\",\"Mod_2\",\"Mod_3\",\"Mod_4\")\n",
        "Loss = (Model_1[0],Model_2[0],Model_3[0],Model_4[0])\n",
        "Accuracy = (Model_1[1],Model_2[1],Model_3[1],Model_4[1])"
      ],
      "metadata": {
        "id": "9EuS-_y0UO7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Loss,Accuracy)\n",
        "for i, txt in enumerate(Models):\n",
        "ax.annotate(txt, (Loss[i],Accuracy[i] ))\n",
        "plt.title(\"Summary for Accuracy and Loss\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZpGk8fQTUQtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusions: From the above graph we can conclude that model 2 is the best among all with higher accuracy and minimum loss, however model model 4 has the highest loss"
      ],
      "metadata": {
        "id": "bk1e1_Dm9Rtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendation: As we can see model 2 is performing best amoung all 4 models hence we should choose model with filters from 32 to 256, 5 Input Layers, Augmented Images and Dropout rate of 0.5"
      ],
      "metadata": {
        "id": "fnM3sI2a9Ui2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase your training sample size. You may pick any amount. Keep the validation and test samples the same as above. Optimize your network (again training from scratch).What performance did you achieve?"
      ],
      "metadata": {
        "id": "auGt3Skn9X6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering Training Sample – 2000"
      ],
      "metadata": {
        "id": "ndYn8DvQ9d-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs\")\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "for category in (\"cat\", \"dog\"):\n",
        "dir = new_base_dir / subset_name / category\n",
        "os.makedirs(dir)\n",
        "fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "for fname in fnames:\n",
        "shutil.copyfile(src=original_dir / fname,\n",
        "dst=dir / fname)\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1250)\n",
        "make_subset(\"test\", start_index=1500, end_index=1750)"
      ],
      "metadata": {
        "id": "0OU6Y4OIUTwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-Processing: Using image_dataset_from_directory to read images"
      ],
      "metadata": {
        "id": "8RJrWFx89mqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"train\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"validation\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"test\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)"
      ],
      "metadata": {
        "id": "1aSfepgyUX8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 2000 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "v_Fyi12B98cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "EwqnNl-_gsMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "4RT8wNrBguDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing the size of the images"
      ],
      "metadata": {
        "id": "AY1MEiI5-B1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "print(\"data batch shape:\", data_batch.shape)\n",
        "print(\"labels batch shape:\", labels_batch.shape)\n",
        "break"
      ],
      "metadata": {
        "id": "KVYoHYriUbVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing the size of the images"
      ],
      "metadata": {
        "id": "cTI-I4BZ-NPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation_1 = keras.Sequential(\n",
        "[\n",
        "layers.RandomFlip(\"horizontal\"),\n",
        "layers.RandomRotation(0.15),\n",
        "layers.RandomZoom(0.25)\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "1NPBKYzPUeMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 5 MaxPooling Operation with Increase in filters from 32 to 256 in 5 Input Layers with the data being used from the Augmented Images and a droput rate of 0.5 (Training Sample - 2000)"
      ],
      "metadata": {
        "id": "1ZORtNn9-kxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation_1(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "u-Viuud7Uh_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Model Summary\n"
      ],
      "metadata": {
        "id": "OJ0SF-dl-8VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KbAj1n-iUoHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model 5"
      ],
      "metadata": {
        "id": "zeBCEYHk_B8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model.compile(loss= \"binary_crossentropy\",\n",
        "optimizer= \"adam\",\n",
        "metrics= ['accuracy'])\n",
        "# Monitoring the best validation loss using Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath = \"model5.keras\",\n",
        "           save_best_only= True,\n",
        "           monitor= \"val_loss\"\n",
        ")\n",
        "# Model Fit\n",
        "Model_5 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 50,\n",
        "validation_data= validation_dataset,\n",
        "callbacks= callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "L7zqvEcQUpNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Training and Validation Accuracy/Loss 2"
      ],
      "metadata": {
        "id": "XVuEpD_r_daL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_5.history[\"accuracy\"]\n",
        "val_accuracy = Model_5.history[\"val_accuracy\"]\n",
        "loss = Model_5.history[\"loss\"]\n",
        "val_loss = Model_5.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\", linestyle=\"dashed\",␣\n",
        "↪label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VCnA3G84U77x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the performance of Model_5 on test set"
      ],
      "metadata": {
        "id": "MwD8ri0y_lIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model5.keras\")\n",
        "Model5_Results = best_model.evaluate(test_dataset)\n",
        "print(f'Loss: {Model5_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model5_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "hDvgeCigVBVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary : The accuracy of the second model, which was only generated with 1000 training examples, was 71%, but the accuracy of the same model increased to 81%, or a 10% improvement, when training samples were increased to 2000."
      ],
      "metadata": {
        "id": "o_amOoI0_qfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 6 Strides Operation with Padding being “Same” with Increase in filters from 32 to 256 in 5 Input Layers with the data being used from the Augmented Images and a droput rate of 0.5 (Training Sample - 2000)"
      ],
      "metadata": {
        "id": "xKkvkrbG_tpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation_1(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, strides=2, activation=\"relu\",␣\n",
        "↪padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, strides=2, activation=\"relu\",␣\n",
        "↪padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, strides=2, activation=\"relu\",␣\n",
        "↪padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, strides=2, activation=\"relu\",␣\n",
        "↪padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, strides=2, activation=\"relu\",␣\n",
        "↪padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "QXjnN0KSVFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model summary"
      ],
      "metadata": {
        "id": "yRw56maC_-lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "kjoutK53VMQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model 6"
      ],
      "metadata": {
        "id": "mcPtKFTzADyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model.compile(loss= \"binary_crossentropy\",\n",
        "optimizer= \"adam\",\n",
        "metrics= ['accuracy'])\n",
        "# Monitoring the best validation loss using Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath = \"model6.keras\",\n",
        "save_best_only= True,\n",
        "monitor= \"val_loss\"\n",
        ")\n",
        "# Model Fit\n",
        "Model_6 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 50,\n",
        "validation_data= validation_dataset,\n",
        "callbacks= callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "9jhb0zQAVPNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Training and Validation Accuracy/Loss"
      ],
      "metadata": {
        "id": "q5M5Fm4aALy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_6.history[\"accuracy\"]\n",
        "val_accuracy = Model_6.history[\"val_accuracy\"]\n",
        "loss = Model_6.history[\"loss\"]\n",
        "val_loss = Model_6.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\", linestyle=\"dashed\",␣\n",
        "↪label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hj8FARB8VTcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model6.keras\")\n",
        "Model6_Results = best_model.evaluate(test_dataset)\n",
        "print(f'Loss: {Model6_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model6_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "usJQ_RM-VanO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary for Question 2: We did try to build 2 more models with training sample being 2000. Now lets compare the loss and Accuracy of 3 models to see which model gives better result"
      ],
      "metadata": {
        "id": "CjrvdlE-ATlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5, training size 1000"
      ],
      "metadata": {
        "id": "ex3O9ZQVAXJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 5: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5, training size 2000"
      ],
      "metadata": {
        "id": "Kn4Ny4qWAZrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 6: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5, training size 2000, Padding being same"
      ],
      "metadata": {
        "id": "dK_kSjbOAb4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_5 = (0.459,0.814)\n",
        "Model_6 = (0.602,0.674)"
      ],
      "metadata": {
        "id": "rX9w_xmCVeka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Models_2 = (\"Mod_2\",\"Mod_5\",\"Mod_6\")\n",
        "Loss_2 = (Model_2[0],Model_5[0],Model_6[0])\n",
        "Accuracy_2 = (Model_2[1],Model_5[1],Model_6[1])"
      ],
      "metadata": {
        "id": "GkV8liebVhdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Loss_2,Accuracy_2)\n",
        "for i, txt in enumerate(Models_2):\n",
        "ax.annotate(txt, (Loss_2[i],Accuracy_2[i] ))\n",
        "plt.title(\"Summary for Accuracy and Loss\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CDhrA5iKVkGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the models' performances were compared, it was found that the model did not gain much from using strides with padding. With the addition of a Max Pooling Layer, Model 5 outperformed the Strides model in accuracy by 14%. Additionally, an improved accuracy of 81% was attained by fine-tuning the network and expanding the training dataset from 1000 to 2000 samples."
      ],
      "metadata": {
        "id": "9m-DQeBUAphF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plotted Models 5 and 6 to answer the second question and provide a visual comparison of their performance. The graphs clearly show that Model 5 had the lowest loss of 45.9% and the highest accuracy of all the models, reaching 81%. The model performed significantly better once the training samples were increased to 2000 and various augmented photos were added."
      ],
      "metadata": {
        "id": "l0C01MdVAtLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tNow change your training sample so that you achieve better performance than those from Steps 1 and 2. This sample size may be larger, or smaller than those in the previous steps. The objective is to find the ideal training sample size to get best prediction results"
      ],
      "metadata": {
        "id": "MbUN_qJ8A2XB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw in above graph that with the increase in training sample size the Accuracy is also increasing hence will increase the sample size to 3000 and 5000 for better performance"
      ],
      "metadata": {
        "id": "N-4pjIZGA8NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Sample 3000"
      ],
      "metadata": {
        "id": "ZCTNI4o5BAB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_1\")\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "for category in (\"cat\", \"dog\"):\n",
        "dir = new_base_dir / subset_name / category\n",
        "os.makedirs(dir)\n",
        "fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "for fname in fnames:\n",
        "shutil.copyfile(src=original_dir / fname,\n",
        "dst=dir / fname)\n",
        "make_subset(\"train\", start_index=0, end_index=1500)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1250)\n",
        "make_subset(\"test\", start_index=1500, end_index=1750)"
      ],
      "metadata": {
        "id": "V5kTASqvVnjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 3000 files belonging to 2 classes.\n"
      ],
      "metadata": {
        "id": "gqFWcZY-hRCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "ZI-2tmGDhU6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset: print(\"data batch shape:\", data_batch.shape) print(\"labels batch shape:\", labels_batch.shape) break"
      ],
      "metadata": {
        "id": "eUeQZzO_hdId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using few of the techniques such as random flip, random zoom, random rotation so as to create augmented versions of the image"
      ],
      "metadata": {
        "id": "WkwWIzN3jzH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation_2 = keras.Sequential( [\n",
        "layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.15), layers.RandomZoom(0.25)\n",
        "]\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yvp5M2t0j9Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 7 MaxPooling Operation with Increase in filters from 32 to 256 in 5 Input Layers with the data being used from the Augmented Images and a droput rate of 0.5 (Training Sample - 3000)"
      ],
      "metadata": {
        "id": "JapAaXrYkBlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs  =  keras.Input(shape=(180,  180,  3)) x = data_augmentation_2(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x  =  layers.Conv2D(filters=32,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2)(x)\n",
        "x  =  layers.Conv2D(filters=64,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2)(x)\n",
        "x  =  layers.Conv2D(filters=128,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2)(x)\n",
        "x  =  layers.Conv2D(filters=256,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2)(x)\n",
        "x  =  layers.Conv2D(filters=256,  kernel_size=3,  activation=\"relu\")(x) x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs  =  layers.Dense(1,  activation=\"sigmoid\")(x)\n",
        "model  =  keras.Model(inputs=inputs,  outputs=outputs)\n"
      ],
      "metadata": {
        "id": "cADALYQVkKGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "S0tF3GlNkQht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling  the  Model\n",
        "model.compile(loss=  \"binary_crossentropy\",\n",
        "optimizer=  \"adam\", metrics=  ['accuracy'])\n",
        "\n",
        "# Monitoring  the  best  validation  loss  using  Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath = \"model7.keras\", save_best_only= True, monitor= \"val_loss\"\n",
        ")\n",
        "# Model Fit\n",
        "Model_7 = model.fit(\n",
        "train_dataset, epochs= 50,\n",
        "validation_data= validation_dataset, callbacks= callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "PzSDYzxOkYEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_7.history[\"accuracy\"] val_accuracy = Model_7.history[\"val_accuracy\"]\n",
        "\n",
        "loss    =    Model_7.history[\"loss\"] val_loss = Model_7.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs,  accuracy,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_accuracy,  color=\"blue\",  linestyle=\"dashed\",␣\n",
        "↪label=\"Validation  Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\") plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ElNCiubXkqU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model7.keras\") Model7_Results = best_model.evaluate(test_dataset) print(f'Loss:  {Model7_Results[0]:.3f}') print(f'Accuracy:  {Model7_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "s3KwSjUfk7TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous Model 6, we attempted to replace the conventional max pooling operation with strides, but the results were not as promising as expected. and in model 7 we used Maxpooling only. Therefore, we are exploring a hybrid approach that combines both max pooling and strides to evaluate the performance of this new model."
      ],
      "metadata": {
        "id": "OHfOVZcdlBLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max pooling is a downsampling operation that reduces the spatial dimensions of the feature map, aiming to capture the most prominent features while discarding less relevant information. On the other hand, strides determine the step rate of the sliding window used to extract and learn the features from the data. This hybrid approach aims to leverage the advantages of both tech- niques, potentially enhancing the model’s ability to capture intricate patterns and features while maintaining computational efficiency."
      ],
      "metadata": {
        "id": "83fsldP9lEOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 8 MaxPooling + Strides of Step-Size 2 Operation with Increase in filters from 32 to 256"
      ],
      "metadata": {
        "id": "Disjb-XRlIUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in 5 Input Layers with the data being used from the Augmented Images and a droput rate of 0.5 (Training Sample - 3000)"
      ],
      "metadata": {
        "id": "sRj6IXUYlMw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs  =  keras.Input(shape=(180,  180,  3)) x = data_augmentation_2(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x  =  layers.Conv2D(filters=32,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2,  strides=2)(x)\n",
        "x  =  layers.Conv2D(filters=64,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2,  strides=2)(x)\n",
        "x  =  layers.Conv2D(filters=128,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2,  strides=2)(x)\n",
        "x  =  layers.Conv2D(filters=256,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2,strides=2)(x)\n",
        "x  =  layers.Conv2D(filters=256,  kernel_size=3,  activation=\"relu\")(x) x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs  =  layers.Dense(1,  activation=\"sigmoid\")(x)\n",
        "model  =  keras.Model(inputs=inputs,  outputs=outputs)\n"
      ],
      "metadata": {
        "id": "NCKn2WTdlPhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling  the  Model\n",
        "model.compile(loss=  \"binary_crossentropy\",\n",
        "optimizer=  \"adam\", metrics=  ['accuracy'])\n",
        "\n",
        "# Monitoring  the  best  validation  loss  using  Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath = \"model8.keras\", save_best_only= True, monitor= \"val_loss\"\n",
        ")\n",
        "# Model Fit\n",
        "Model_8 = model.fit(\n",
        "train_dataset, epochs= 50,\n",
        "validation_data= validation_dataset, callbacks= callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "XtRhTMMzlb_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_8.history[\"accuracy\"] val_accuracy = Model_8.history[\"val_accuracy\"]\n",
        "\n",
        "loss    =    Model_8.history[\"loss\"] val_loss = Model_8.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs,  accuracy,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_accuracy,  color=\"blue\",  linestyle=\"dashed\",␣\n",
        "↪label=\"Validation  Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\") plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Azk_wHIslioN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model8.keras\") Model8_Results = best_model.evaluate(test_dataset) print(f'Loss:  {Model8_Results[0]:.3f}') print(f'Accuracy:  {Model8_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "A0u08PGhlnuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 9 MaxPooling + Strides of Step-Size 2 with Padding turned on Operation with Increase in filters from 32 to 512 in 5 Input Layers with the data being used from the Augmented Images and a droput rate of 0.5 (Training Sample - 3000)"
      ],
      "metadata": {
        "id": "9wI8_SY-lqYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs  =  keras.Input(shape=(180,  180,  3)) x = data_augmentation_2(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x  =  layers.Conv2D(filters=32,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2,  strides=2,  padding=\"same\")(x) x  =  layers.Conv2D(filters=64,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2,  strides=2,  padding=\"same\")(x) x  =  layers.Conv2D(filters=128,  kernel_size=3,  activation=\"relu\")(x)\n",
        "x  =  layers.MaxPooling2D(pool_size=2,  strides=2,  padding=\"same\")(x) x  =  layers.Conv2D(filters=256,  kernel_size=3,  activation=\"relu\")(x) x  =  layers.MaxPooling2D(pool_size=2,strides=2,  padding=\"same\")(x)\n",
        "x  =  layers.Conv2D(filters=512,  kernel_size=3,  activation=\"relu\")(x) x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs  =  layers.Dense(1,  activation=\"sigmoid\")(x)\n",
        "model  =  keras.Model(inputs=inputs,  outputs=outputs)\n"
      ],
      "metadata": {
        "id": "-3-SItPdltBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rHKCNTurlzjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling  the  Model\n",
        "model.compile(loss=  \"binary_crossentropy\",\n",
        "optimizer=  \"adam\", metrics=  ['accuracy'])\n",
        "\n",
        "# Monitoring  the  best  validation  loss  using  Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath = \"model9.keras\", save_best_only= True, monitor= \"val_loss\"\n",
        ")\n",
        "# Model Fit\n",
        "Model_9 = model.fit(\n",
        "train_dataset, epochs= 50,\n",
        "validation_data= validation_dataset, callbacks= callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "xyk-PJr1l3Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_9.history[\"accuracy\"] val_accuracy = Model_9.history[\"val_accuracy\"]\n",
        "\n",
        "loss    =    Model_9.history[\"loss\"] val_loss = Model_9.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs,  accuracy,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_accuracy,  color=\"blue\",  linestyle=\"dashed\",␣\n",
        "↪label=\"Validation  Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\") plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "UqWKV1IHl-RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model9.keras\") Model9_Results = best_model.evaluate(test_dataset) print(f'Loss:  {Model9_Results[0]:.3f}') print(f'Accuracy:  {Model9_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "1Bemw8nzmaOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see which of the models have best performance when the training sample was set to 3000. Note: Here models 8 and 9 were trained differently with strides being used with maxpooling and strides being used with maxpooling and padding turned on."
      ],
      "metadata": {
        "id": "7qKSfPZLmNUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 6: trides Operation with Padding being “Same” ,filters from 32 to 512, 5 Input Layers, droput rate of 0.5, Training Sample - 3000"
      ],
      "metadata": {
        "id": "OnwVwrXjmQWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "best_model = keras.models.load_model(\"model9.keras\") Model9_Results = best_model.evaluate(test_dataset) print(f'Loss:  {Model9_Results[0]:.3f}') print(f'Accuracy:  {Model9_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "bCRJH18CmUOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 8: MaxPooling + Strides of Step-Size 2,filters from 32 to 512, 5 Input Layers, droput rate of 0.5, Training Sample - 3000"
      ],
      "metadata": {
        "id": "UEtD4SttmfHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 9: MaxPooling + Strides of Step-Size 2 with Padding turned on,filters from 32 to 512, 5 Input Layers, droput rate of 0.5, Training Sample - 3000"
      ],
      "metadata": {
        "id": "BhrPAGwnmiFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_7 = (0.69,0.500)\n",
        "Model_8 = (0.457,0.812)\n",
        "Model_9 = (0.409,0.832)\n"
      ],
      "metadata": {
        "id": "RbYkD901mtbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Models_3  =  (\"Mod_6\",\"Mod_7\",\"Mod_8\",\"Mod_9\")\n",
        "Loss_3 = (Model_6[0],Model_7[0],Model_8[0],Model_9[0]) Accuracy_3 = (Model_6[1],Model_7[1],Model_8[1],Model_9[1])\n",
        "\n",
        "fig, ax = plt.subplots() ax.scatter(Loss_3,Accuracy_3)\n",
        "for i, txt in enumerate(Models_3): ax.annotate(txt, (Loss_3[i],Accuracy_3[i] ))\n",
        "plt.title(\"Summary  for  Accuracy  and  Loss\") plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TReDnC7Tm0ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can clearly see that the model which was built with 5 layers using maxpooling along with strides and padding on was giving the highest accuracy i.e. 83.2 % with least loss amoung the other 2 models i.e. 40.9%."
      ],
      "metadata": {
        "id": "jgzss4dJm320"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are increasing the training sample to 5000 and building a model from scratch to check it’s performance on the unseen data."
      ],
      "metadata": {
        "id": "eX2YWbRnm9uL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Sample - 5000"
      ],
      "metadata": {
        "id": "vhEWkJTsnCVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "original_dir = pathlib.Path(\"train\") new_base_dir = pathlib.Path(\"cats_vs_dogs_2\")\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "for category in (\"cat\", \"dog\"):\n",
        "dir = new_base_dir / subset_name / category os.makedirs(dir)\n",
        "fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "for fname in fnames: shutil.copyfile(src=original_dir  /  fname,\n",
        "dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\",  start_index=0,  end_index=2500) make_subset(\"validation\",  start_index=1000,  end_index=1250) make_subset(\"test\",  start_index=1500,  end_index=1750)\n"
      ],
      "metadata": {
        "id": "HbHVLBgVnHvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"train\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"validation\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"test\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)"
      ],
      "metadata": {
        "id": "iPH-E5yAVr-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 5000 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "q65dpiCQnN8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "x6byHIURnXnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 500 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "iH7AmQ81nZvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "print(\"data batch shape:\", data_batch.shape)\n",
        "print(\"labels batch shape:\", labels_batch.shape)\n",
        "break"
      ],
      "metadata": {
        "id": "AAA-_miiV2dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data batch shape: (32, 180, 180, 3)labels batch shape: (32,)"
      ],
      "metadata": {
        "id": "PPbEzejendgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation_2 = keras.Sequential(\n",
        "[\n",
        "layers.RandomFlip(\"horizontal\"),\n",
        "layers.RandomRotation(0.15),\n",
        "layers.RandomZoom(0.25)\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "5lDNWX9XWDjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model - 10 MaxPooling Operation with Increase in filters from 32 to 256 in 5 Input Layers with the data being used from the Augmented Images and a droput rate of 0.5 (Training Sample - 5000)"
      ],
      "metadata": {
        "id": "GSjS_DoVnkJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation_2(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "hMYGP9yqWEX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "09oP8nmeWIbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model.compile(loss= \"binary_crossentropy\",\n",
        "optimizer= \"adam\",\n",
        "metrics= ['accuracy'])\n",
        "# Monitoring the best validation loss using Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "            filepath = \"model7.keras\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "# Model Fit\n",
        "Model_7 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 50,\n",
        "validation_data= validation_dataset,\n",
        "callbacks= callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "vc-FBLCiWLYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = Model_7.history[\"accuracy\"]\n",
        "val_accuracy = Model_7.history[\"val_accuracy\"]\n",
        "loss = Model_7.history[\"loss\"]\n",
        "val_loss = Model_7.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\", linestyle=\"dashed\",␣\n",
        "↪label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qzuH4xoyWdwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"model7.keras\")\n",
        "Model7_Results = best_model.evaluate(test_dataset)\n",
        "print(f'Loss: {Model7_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model7_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "xM8yE0XEWlpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary for Question 3,"
      ],
      "metadata": {
        "id": "KBa5Sahun4w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We built four models, three of which were trained using a 3000 sample size. The best-performing model has an accuracy of 83.2%. Interestingly, the accuracy increased to 88.4% when we increased the training sample to 5000. Thus, we conclude that a training sample size of 5000 significantly improves the model's performance. As for the likely cause of the validation loss being less than the training loss, the split approach that was used is probably a factor.\n",
        "\n"
      ],
      "metadata": {
        "id": "87P8idyIn6va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the validation and test sets are fixed at 500 apiece, but the training sample is almost five thousand times larger. Furthermore, it is crucial to recognize that regularizations like dropout or L1 and L2 regularizers are important during training and contribute to the training loss calculation. On the other hand, these regularizers are turned off during the validation or test phase, which could result in a smaller loss than the training loss."
      ],
      "metadata": {
        "id": "zpexByMHn9Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tRepeat Steps 1-3, but now using a pretrained network. The sample sizes you use in Steps 2 and 3 for the pretrained network may be the same or different from those using the network where you trained from scratch. Again, use any and all optimization techniques to get best performance\n"
      ],
      "metadata": {
        "id": "HZT3l3P2oAda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leveraging a Pre-Trained Model - VGG16"
      ],
      "metadata": {
        "id": "iRu-Y8wGoF-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG - Model 1 (1000 Training Samples)"
      ],
      "metadata": {
        "id": "Z0RggZ5VoH2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base = keras.applications.vgg16.VGG16( weights=\"imagenet\",\n",
        "include_top=False, input_shape=(180,  180,  3))\n"
      ],
      "metadata": {
        "id": "E2ZUvPlFoLKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " conv_base.summary()"
      ],
      "metadata": {
        "id": "xMDUC5OQoOIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting VGG  16  features  and  Labels\n",
        "def get_features_and_labels(dataset): all_features = []\n",
        "all_labels = []\n",
        "for images, labels in dataset:\n",
        "preprocessed_images = keras.applications.vgg16.preprocess_input(images) features = conv_base.predict(preprocessed_images) all_features.append(features)\n",
        "all_labels.append(labels)\n",
        "return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "train_features,   train_labels   =\tget_features_and_labels(train_dataset) val_features, val_labels =\tget_features_and_labels(validation_dataset) test_features,   test_labels   =\tget_features_and_labels(test_dataset)\n"
      ],
      "metadata": {
        "id": "PzPODd8DoSlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "metadata": {
        "id": "oS392VZ-oZQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG - Model 1 Dense Layer with 256 Nodes and Droput Rate of 0.5 and optimizer being rmsprop with the Original Images"
      ],
      "metadata": {
        "id": "vWo8ENglob6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining and  Training  the  densely  connected  classifier # The last dense stacked layer and the classifier\n",
        "inputs  =  keras.Input(shape=(5,  5,  512)) x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Compiling the  Model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Using CallBacks  to  monitor  the  best  val_loss\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath=\"vgg_model1.keras\", save_best_only=True, monitor=\"val_loss\")\n",
        "\n",
        "# Model Fit\n",
        "VGG_Model_1 = model.fit(\n",
        "train_features, train_labels, epochs= 30,\n",
        "validation_data= (val_features, val_labels), callbacks= callbacks)\n"
      ],
      "metadata": {
        "id": "8XrcmbFMofM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = VGG_Model_1.history[\"accuracy\"]\n",
        "val_acc = VGG_Model_1.history[\"val_accuracy\"]\n",
        "\n",
        "loss   =   VGG_Model_1.history[\"loss\"] val_loss = VGG_Model_1.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs,  acc,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_acc,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and validation Loss\") plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F9G_dw3VoqAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"vgg_model1.keras\") VGG_Model_1_Results = best_model.evaluate(test_features,test_labels) print(f'Loss:  {VGG_Model_1_Results[0]:.3f}')\n",
        "print(f'Accuracy:  {VGG_Model_1_Results[1]:.3f}')\n"
      ],
      "metadata": {
        "id": "Q0V3VJlSovnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG - Model 2 (1000 Training Samples)"
      ],
      "metadata": {
        "id": "qKZCiKy8oxpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only the densely linked networks and the classifier are permitted to modify their weights while the pre-trained model is set to maintain its current weights throughout training."
      ],
      "metadata": {
        "id": "hU7HsaZco0xO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method helps avoid overfitting because the pre-trained model doesn't change, giving the model a solid base. Furthermore, freezing the pre-trained model training might be very helpful when working with limited training data and processing resources."
      ],
      "metadata": {
        "id": "NJW0IQjXo27j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can output the list of trainable weights both before and after freezing the pre-trained model to show the effect of this setting."
      ],
      "metadata": {
        "id": "zp9Lj01Po5LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before  Freezing\n",
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "\"before freezing the conv base:\", len(conv_base.trainable_weights))\n"
      ],
      "metadata": {
        "id": "umiwKh31o8I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After  Freezing\n",
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "\"after freezing the conv base:\", len(conv_base.trainable_weights))\n"
      ],
      "metadata": {
        "id": "gn2j2foTpANo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "V61IKTHwpD1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG - Model 2 Dense Layer with 256 Nodes and Droput Rate of 0.5 and optimizer being rmsprop with the Augmented Images"
      ],
      "metadata": {
        "id": "AE3mYFDGpF3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data  Augmentation  -Adding  a  data  augmentation  stage  to  provide  augmented␣\n",
        "↪training  samples  and  a  classifier  to  the  convolutional  base\n",
        "data_augmentation = keras.Sequential( [\n",
        "layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1), layers.RandomZoom(0.2),\n",
        "]\n",
        ")\n",
        "\n",
        "# Adding the Classifier and Dense Network inputs  =  keras.Input(shape=(180,  180,  3)) x = data_augmentation(inputs)\n",
        "x =  keras.applications.vgg16.preprocess_input(x) x = conv_base(x)\n",
        "x   =    layers.Flatten()(x) x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs  =  layers.Dense(1,  activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs)\n",
        "# Compiling  the  Model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Using  CallBacks  to  monitor  the  best  val_loss\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath= \"vgg_model2.keras\", save_best_only= True, monitor= \"val_loss\")\n",
        "\n",
        "# Model Fit\n",
        "VGG_Model_2 = model.fit(\n",
        "train_dataset, epochs= 30,\n",
        "validation_data=validation_dataset, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "qPYfkbwRpJLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = VGG_Model_2.history[\"accuracy\"]\n",
        "val_acc = VGG_Model_2.history[\"val_accuracy\"]\n",
        "\n",
        "loss   =   VGG_Model_2.history[\"loss\"] val_loss = VGG_Model_2.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs,  acc,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_acc,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and validation Loss\") plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GCUZYgX3pUYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"vgg_model2.keras\") VGG_Model_2_Results= best_model.evaluate(test_dataset) print(f'Loss:  {VGG_Model_2_Results[0]:.3f}') print(f'Accuracy:  {VGG_Model_2_Results[1]:.3f}')"
      ],
      "metadata": {
        "id": "e9TaI-IvpYFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tuning the VGG_Model_2"
      ],
      "metadata": {
        "id": "8NOLiQEJpbNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]: layer.trainable = False\n"
      ],
      "metadata": {
        "id": "33BhWPnlpck1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to realize that pre-trained networks are trained to handle a variety of use cases and classifications, and are not only employed for solitary picture classification tasks. The network's first layers are good at gathering broad data, while its later levels usually focus on extracting features that are unique to the issue at hand. By choosing to freeze the first few layers, we may effectively avoid overfitting and allow the model to incorporate more complex information related to our particular categorization problem. The model is encouraged to concentrate on understanding the subtle nuances of the target classification problem by using this strategic approach."
      ],
      "metadata": {
        "id": "p7O6b97ypgKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=keras.optimizers.RMSprop(learning_rate=1e-5), metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath=\"fine_tuning_vgg_model2.keras\", save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "\n",
        "FineTuned_VGG_Model_2 = model.fit(\n",
        "train_dataset, epochs=30,\n",
        "validation_data=validation_dataset, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "_oGMVD4OpkzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = FineTuned_VGG_Model_2.history[\"accuracy\"]\n",
        "val_acc = FineTuned_VGG_Model_2.history[\"val_accuracy\"]\n",
        "\n",
        "loss = FineTuned_VGG_Model_2.history[\"loss\"] val_loss = FineTuned_VGG_Model_2.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs,  acc,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_acc,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and validation Loss\") plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PB0-RtzpppMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"fine_tuning_vgg_model2.keras\") FineTuned_VGG_Model_2_Results = best_model.evaluate(test_dataset) print(f\"Loss:   {FineTuned_VGG_Model_2_Results[0]:.3f}\") print(f\"Accuracy: {FineTuned_VGG_Model_2_Results[1]:.3f}\")"
      ],
      "metadata": {
        "id": "oIz2y71uptDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pre-trained network VGG16, we created three models for the examination of the above two VGG16 models. Interestingly, we found that accuracy increased when the pre-trained network was frozen in its first layers and prevented from altering its weights during training. As such, we want to use the same methods with a training sample size of 5000 to construct two models."
      ],
      "metadata": {
        "id": "LSt4XmsZpvki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG - Model 3 (5000 Training Samples)"
      ],
      "metadata": {
        "id": "1BmcMKBEpySo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base= keras.applications.vgg16.VGG16( weights=\"imagenet\", include_top=False)\n"
      ],
      "metadata": {
        "id": "8fjv1Vx9p0k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = False"
      ],
      "metadata": {
        "id": "Syi5VI3rqBk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " conv_base.summary()"
      ],
      "metadata": {
        "id": "KdM84_0Yp9AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = image_dataset_from_directory( new_base_dir / \"train\",\n",
        "image_size=(180,  180), batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory( new_base_dir / \"validation\",\n",
        "image_size=(180,  180), batch_size=32)\n",
        "test_dataset = image_dataset_from_directory( new_base_dir / \"test\",\n",
        "image_size=(180,  180), batch_size=32)\n"
      ],
      "metadata": {
        "id": "Esi8CgQ_qEJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 5000 files belonging to 2 classes. Found 500 files belonging to 2 classes. Found 500 files belonging to 2 classes"
      ],
      "metadata": {
        "id": "0embGffsqKcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "data_augmentation_4 = keras.Sequential( [\n",
        "layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.15), layers.RandomZoom(0.25),\n",
        "]\n",
        ")\n",
        "\n",
        "# Adding the Classifier and Dense Network inputs  =  keras.Input(shape=(180,  180,  3)) x = data_augmentation_4(inputs)\n",
        "x =  keras.applications.vgg16.preprocess_input(x) x = conv_base(x)\n",
        "x   =    layers.Flatten()(x) x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs  =  layers.Dense(1,  activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs)\n",
        "# Compiling  the  Model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "# Using CallBacks  to  monitor  the  best  val_loss\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath= \"vgg_model3.keras\", save_best_only= True, monitor= \"val_loss\")\n",
        "\n",
        "# Model Fit\n",
        "VGG_Model_3 = model.fit(\n",
        "train_dataset, epochs= 50,\n",
        "validation_data=validation_dataset, callbacks=callbacks)\n",
        "\n"
      ],
      "metadata": {
        "id": "_ta3qh2DqSWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = VGG_Model_3.history[\"accuracy\"]\n",
        "val_acc = VGG_Model_3.history[\"val_accuracy\"]\n",
        "\n",
        "loss   =   VGG_Model_3.history[\"loss\"] val_loss = VGG_Model_3.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs,  acc,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_acc,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\") plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uA3zbY67qa6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"vgg_model3.keras\") VGG_Model_3_Results = best_model.evaluate(test_dataset) print(f\"Loss: {VGG_Model_3_Results[0]:.3f}\") print(f\"Accuracy: {VGG_Model_3_Results[1]:.3f}\")"
      ],
      "metadata": {
        "id": "RyM9Q1M6qhF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tunning VGG_Model_3 (Training Samples - 5000)"
      ],
      "metadata": {
        "id": "MO1bmktRqkkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have decided to freeze the first four layers in order to optimize VGG_Model3. By using this tactic, we hope to stop the model from overfitting and free it up to focus just on identifying the unique characteristics that are pertinent to our specific categorization task.As such, we have simultaneously made sure that the first four layers stay frozen and configured the pre-trained layers to remain unchanged during training. The model performs better with these improvements applied, especially when working with a training sample size of 5000."
      ],
      "metadata": {
        "id": "2UICfpBXqnHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]: layer.trainable = False\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath=\"fine_tuning_vgg_model3.keras\", save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "\n",
        "FineTuned_VGG_Model_3 = model.fit(\n",
        "train_dataset, epochs=50,\n",
        "validation_data=validation_dataset, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "FJunTWoGqjNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = FineTuned_VGG_Model_3.history[\"accuracy\"]\n",
        "val_acc = FineTuned_VGG_Model_3.history[\"val_accuracy\"]\n",
        "\n",
        "loss = FineTuned_VGG_Model_3.history[\"loss\"] val_loss = FineTuned_VGG_Model_3.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs,  acc,  color=\"grey\",  label=\"Training  Accuracy\") plt.plot(epochs,  val_acc,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and validation Accuracy\") plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,  loss,  color=\"grey\",  label=\"Training  Loss\")\n",
        "plt.plot(epochs,  val_loss,  color=\"blue\",  linestyle=\"dashed\",  label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and validation Loss\") plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NI3HVWWLqyrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"fine_tuning_vgg_model3.keras\") FineTuned_VGG_Model_3_Results = best_model.evaluate(test_dataset) print(f\"Loss:   {FineTuned_VGG_Model_3_Results[0]:.3f}\") print(f\"Accuracy: {FineTuned_VGG_Model_3_Results[1]:.3f}\")"
      ],
      "metadata": {
        "id": "QbkCoSDQq60y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After building a total of 15 models—two of which are optimized copies of the original models—we are now prepared to do a comparative study in order to identify the top models in two different categories: Pre-Trained Models and Scratch Models. Our immediate goal is to assess which scratch-built model performs the best. The evaluation comprises a comparison of the accuracy and loss metrics of the ten models that were constructed using four distinct training data. Finding the ideal training sample size for the classification of cats and dogs is the main goal.\n",
        "\n"
      ],
      "metadata": {
        "id": "8TJhnf3Hq9J8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: filters from 32 to 256, 5 Input Layers\n",
        "Model 2: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5 Model 3: filters from 32 to 512, 6 Input Layers, Augmented Images and Dropout rate of 0.5 Model 4: filters from 64 to 1024, 5 Input Layers, Augmented Images and Dropout rate of 0.6\n",
        "Model 5: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5, training size 2000\n",
        "Model 6: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5, training size 2000, Padding being same\n",
        "Model 7: MaxPooling Operation,filters from 32 to 512, 5 Input Layers, Augmented Images, droput rate of 0.5, Training Sample - 3000\n",
        "Model 8: MaxPooling + Strides of Step-Size 2,filters from 32 to 512, 5 Input Layers, Augmented Images, droput rate of 0.5, Training Sample - 3000\n",
        "Model 9: MaxPooling + Strides of Step-Size 2 with Padding turned on,filters from 32 to 512, 5 Input Layers,Augmented Images, droput rate of 0.5, Training Sample - 3000\n",
        "Model 10: filters from 32 to 512, 5 Input Layers,Augmented Images, droput rate of 0.5, Training\n"
      ],
      "metadata": {
        "id": "KrGQfIw0q_1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample - 500\n"
      ],
      "metadata": {
        "id": "5MreznrRrHlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scratch  Models\n",
        "Model_1 = (0.653, 0.696)\n",
        "Model_2 = (0.588, 0.714)\n",
        "Model_3 = (0.620,0.674)\n",
        "Model_4 = (0.604, 0.680)\n",
        "Model_5 = (0.459,0.814)\n",
        "Model_6 = (0.602,0.674)\n",
        "Model_7 = (0.69,0.500)\n",
        "Model_8 = (0.457,0.812)\n",
        "Model_9 = (0.409,0.832)\n",
        "Model_10 = (0.283,0.884)\n"
      ],
      "metadata": {
        "id": "Mnk7kbuhrOEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "Models_4  =␣\n",
        "↪(\"Mod_1\",\"Mod_2\",\"Mod_3\",\"Mod_4\",\"Mod_5\",\"Mod_6\",\"Mod_7\",\"Mod_8\",\"Mod_9\",\"Mod_10\")\n",
        "Loss_4  =␣\n",
        "↪(Model_1[0],Model_2[0],Model_3[0],Model_4[0],Model_5[0],Model_6[0],Model_7[0],Model_8[0],Mo\n",
        "Accuracy_4  =␣\n",
        "↪(Model_1[1],Model_2[1],Model_3[1],Model_4[1],Model_5[1],Model_6[1],Model_7[1],Model_8[1],Mo\n",
        "\n",
        "fig, ax = plt.subplots() ax.scatter(Loss_4,Accuracy_4)\n",
        "for i, txt in enumerate(Models_4): ax.annotate(txt, (Loss_4[i],Accuracy_4[i] ))\n",
        "plt.title(\"Summary  for  Accuracy  and  Loss\") plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kJ7dT2X8rQBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Out of all the scratch models, Model_10, which was trained on 5000 samples, turned out to be the best model with an impressive 88.4% accuracy and a 28.3% loss on the test set. A five-layer architecture with filters ranging from 32 to 256 was used to build Model 10. Augmented photos were integrated into the model during training, along with a max-pooling layer and a 0.5 dropout rate.\n"
      ],
      "metadata": {
        "id": "Soco8GK7rWoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "following, we created five models with the pre-trained vgg16 network. The first three were created with a sample size of 1000 and an optimizer named rmsprop, while the following two were created with a sample size of 5000 and an optimizer named Adam."
      ],
      "metadata": {
        "id": "bOoqOqxCrZt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG 1: filters from 32 to 256, 5 Input Layers\n",
        "VGG 2: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5 VGG 3: filters from 32 to 512, 6 Input Layers, Augmented Images and Dropout rate of 0.5 VGG 4: VGG - Model 3 (5000 Training Samples)\n",
        "VGG 5: Fine Tunning VGG_Model_3 (Training Samples - 5000)\n"
      ],
      "metadata": {
        "id": "9Y8NIAQNrb7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Trained  Models\n",
        "VGG_Model_1 = (0.00,100)\n",
        "VGG_Model_2 = (2.6,99.8)\n",
        "FineTuned_VGG_Model_2 = (0.00,100)\n",
        "VGG_Model_3 = (13.4,98.4)\n",
        "FineTuned_VGG_Model_3 = (0.00,100)\n",
        "Models_5  =  (\"VGG_1\",\"VGG_2\",\"VGG_3\",\"VGG_4\",\"VGG_5\")\n",
        "Loss_5  =␣\n",
        "↪(VGG_Model_1[0],VGG_Model_2[0],FineTuned_VGG_Model_2[0],VGG_Model_3[0],FineTuned_VGG_Model_\n",
        "Accuracy_5  =␣\n",
        "↪(VGG_Model_1[1],VGG_Model_2[1],FineTuned_VGG_Model_2[1],VGG_Model_3[1],FineTuned_VGG_Model_\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots() ax.scatter(Loss_5,Accuracy_5)\n",
        "for i, txt in enumerate(Models_5): ax.annotate(txt, (Loss_5[i],Accuracy_5[i] ))\n",
        "plt.title(\"Summary  for  Accuracy  and  Loss\") plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y0mHDkJzrfOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = FineTuned_VGG_Model_2.history[\"accuracy\"]\n",
        "val_acc = FineTuned_VGG_Model_2.history[\"val_accuracy\"]\n",
        "loss = FineTuned_VGG_Model_2.history[\"loss\"]\n",
        "val_loss = FineTuned_VGG_Model_2.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ZsvaKAuaDP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"fine_tuning_vgg_model2.keras\")\n",
        "FineTuned_VGG_Model_2_Results = best_model.evaluate(test_dataset)\n",
        "print(f\"Loss: {FineTuned_VGG_Model_2_Results[0]:.3f}\")\n",
        "print(f\"Accuracy: {FineTuned_VGG_Model_2_Results[1]:.3f}\")"
      ],
      "metadata": {
        "id": "rLSyJKAAaHmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pre-trained network VGG16, we created three models for the examination of the above two VGG16 models. Interestingly, we found that accuracy increased when the pre-trained network was frozen in its first layers and prevented from altering its weights during training. As such, we want to use the same methods with a training sample size of 5000 to construct two models."
      ],
      "metadata": {
        "id": "ySA2MCB-F053"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG - Model 3 (5000 Training Samples)"
      ],
      "metadata": {
        "id": "BsVksHQdF3DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base= keras.applications.vgg16.VGG16(\n",
        "weights=\"imagenet\",\n",
        "include_top=False)\n",
        "conv_base.trainable = False"
      ],
      "metadata": {
        "id": "tP06rr9naLyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "B5_4ETb5aQvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"train\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"validation\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "new_base_dir / \"test\",\n",
        "image_size=(180, 180),\n",
        "batch_size=32)"
      ],
      "metadata": {
        "id": "weDxU6DRaVMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 5000 files belonging to 2 classes. Found 500 files belonging to 2 classes. Found 500 files belonging to 2 classes."
      ],
      "metadata": {
        "id": "jUbojE9HFg62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "data_augmentation_4 = keras.Sequential(\n",
        "[\n",
        "layers.RandomFlip(\"horizontal\"),\n",
        "layers.RandomRotation(0.15),\n",
        "layers.RandomZoom(0.25),\n",
        "]\n",
        ")\n",
        "# Adding the Classifier and Dense Network\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation_4(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "# Compiling the Model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "# Using CallBacks to monitor the best val_loss\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath= \"vgg_model3.keras\",\n",
        "save_best_only= True,\n",
        "monitor= \"val_loss\")\n",
        "# Model Fit\n",
        "VGG_Model_3 = model.fit(\n",
        "train_dataset,\n",
        "epochs= 50,\n",
        "validation_data=validation_dataset,\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "id": "wVfjali0aY-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = VGG_Model_3.history[\"accuracy\"]\n",
        "val_acc = VGG_Model_3.history[\"val_accuracy\"]\n",
        "loss = VGG_Model_3.history[\"loss\"]\n",
        "val_loss = VGG_Model_3.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GxYhxVi6anr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"vgg_model3.keras\")\n",
        "VGG_Model_3_Results = best_model.evaluate(test_dataset)\n",
        "print(f\"Loss: {VGG_Model_3_Results[0]:.3f}\")\n",
        "print(f\"Accuracy: {VGG_Model_3_Results[1]:.3f}\")"
      ],
      "metadata": {
        "id": "s5EHk5qZauQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tunning VGG_Model_3 (Training Samples - 5000)"
      ],
      "metadata": {
        "id": "DNFPulsdFGOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have decided to freeze the first four layers in order to optimize VGG_Model3. By using this tactic, we hope to stop the model from overfitting and free it up to focus just on identifying the unique characteristics that are pertinent to our specific categorization task.As such, we have simultaneously made sure that the first four layers stay frozen and configured the pre-trained layers to remain unchanged during training. The model performs better with these improvements applied, especially when working with a training sample size of 5000."
      ],
      "metadata": {
        "id": "ZXmDyHHsFIyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "layer.trainable = False\n"
      ],
      "metadata": {
        "id": "5iRS2R4fayIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
        "metrics=[\"accuracy\"])\n",
        "callbacks = ModelCheckpoint(\n",
        "filepath=\"fine_tuning_vgg_model3.keras\",\n",
        "save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "FineTuned_VGG_Model_3 = model.fit(\n",
        "train_dataset,\n",
        "epochs=50,\n",
        "validation_data=validation_dataset,\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "id": "nrZKudxya6p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = FineTuned_VGG_Model_3.history[\"accuracy\"]\n",
        "val_acc = FineTuned_VGG_Model_3.history[\"val_accuracy\"]\n",
        "loss = FineTuned_VGG_Model_3.history[\"loss\"]\n",
        "val_loss = FineTuned_VGG_Model_3.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, color=\"grey\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Accuracy\")\n",
        "plt.title(\"Training and validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color=\"grey\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation␣\n",
        "↪Loss\")\n",
        "plt.title(\"Training and validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m-zAVfCLa75R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"fine_tuning_vgg_model3.keras\")\n",
        "FineTuned_VGG_Model_3_Results = best_model.evaluate(test_dataset)\n",
        "print(f\"Loss: {FineTuned_VGG_Model_3_Results[0]:.3f}\")\n",
        "print(f\"Accuracy: {FineTuned_VGG_Model_3_Results[1]:.3f}\")"
      ],
      "metadata": {
        "id": "n3fgTBlcbAn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After building a total of 15 models—two of which are optimized copies of the original models—we are now prepared to do a comparative study in order to identify the top models in two different categories: Pre-Trained Models and Scratch Models. Our immediate goal is to assess which scratch-built model performs the best. The evaluation comprises a comparison of the accuracy and loss metrics of the ten models that were constructed using four distinct training data. Finding the ideal training sample size for the classification of cats and dogs is the main goal."
      ],
      "metadata": {
        "id": "0muL7RVzEZtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: filters from 32 to 256, 5 Input Layers\n",
        "Model 2: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5 Model 3: filters from 32 to 512, 6 Input Layers, Augmented Images and Dropout rate of 0.5 Model 4: filters from 64 to 1024, 5 Input Layers, Augmented Images and Dropout rate of 0.6\n",
        "Model 5: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5, training size 2000\n",
        "Model 6: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5, training size 2000, Padding being same\n",
        "Model 7: MaxPooling Operation,filters from 32 to 512, 5 Input Layers, Augmented Images, droput rate of 0.5, Training Sample - 3000\n",
        "Model 8: MaxPooling + Strides of Step-Size 2,filters from 32 to 512, 5 Input Layers, Augmented Images, droput rate of 0.5, Training Sample - 3000\n",
        "Model 9: MaxPooling + Strides of Step-Size 2 with Padding turned on,filters from 32 to 512, 5 Input Layers,Augmented Images, droput rate of 0.5, Training Sample - 3000\n",
        "Model 10: filters from 32 to 512, 5 Input Layers,Augmented Images, droput rate of 0.5, Training\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rc2KHzjKEdIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample - 500"
      ],
      "metadata": {
        "id": "6ZO7lzbUEpMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of all the scratch models, Model_10, which was trained on 5000 samples, turned out to be the best model with an impressive 88.4% accuracy and a 28.3% loss on the test set. A five-layer architecture with filters ranging from 32 to 256 was used to build Model 10. Augmented photos were integrated into the model during training, along with a max-pooling layer and a 0.5 dropout rate."
      ],
      "metadata": {
        "id": "VXu4jjuXECna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "following, we created five models with the pre-trained vgg16 network. The first three were created with a sample size of 1000 and an optimizer named rmsprop, while the following two were created with a sample size of 5000 and an optimizer named Adam."
      ],
      "metadata": {
        "id": "jeFHM6l6EGgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG 1: filters from 32 to 256, 5 Input Layers\n",
        "VGG 2: filters from 32 to 256, 5 Input Layers, Augmented Images and Droput rate of 0.5 VGG 3: filters from 32 to 512, 6 Input Layers, Augmented Images and Dropout rate of 0.5 VGG 4: VGG - Model 3 (5000 Training Samples)\n",
        "VGG 5: Fine Tunning VGG_Model_3 (Training Samples - 5000)\n"
      ],
      "metadata": {
        "id": "V_7CUtkaEI9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Trained Models\n",
        "VGG_Model_1 = (0.00,100)\n",
        "VGG_Model_2 = (2.6,99.8)\n",
        "FineTuned_VGG_Model_2 = (0.00,100)\n",
        "VGG_Model_3 = (13.4,98.4)\n",
        "FineTuned_VGG_Model_3 = (0.00,100)"
      ],
      "metadata": {
        "id": "Uuoojv6ubL2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Models_5 = (\"VGG_1\",\"VGG_2\",\"VGG_3\",\"VGG_4\",\"VGG_5\")\n",
        "Loss_5 =␣\n",
        "↪(VGG_Model_1[0],VGG_Model_2[0],FineTuned_VGG_Model_2[0],VGG_Model_3[0],FineTuned_VGG_Model_3[Accuracy_5 =␣\n",
        "↪(VGG_Model_1[1],VGG_Model_2[1],FineTuned_VGG_Model_2[1],VGG_Model_3[1],FineTuned_VGG_Model_3[fig, ax = plt.subplots()\n",
        "ax.scatter(Loss_5,Accuracy_5)\n",
        "for i, txt in enumerate(Models_5):\n",
        "ax.annotate(txt, (Loss_5[i],Accuracy_5[i] ))\n",
        "plt.title(\"Summary for Accuracy and Loss\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EQu-OJhibSkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When it came to pre-trained models, the two best performers were Fine- Tuned_VGG_Model_2 and FineTuned_VGG_Model_3, or Model_5and model_3.\n"
      ],
      "metadata": {
        "id": "DzeXhyfaDiSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with a meagre 0.00% loss and an astounding 100% accuracy. This model was built with 2000 and 5000 training samples, and it was tuned at a learning rate of 0.000001 using the Adam optimizer."
      ],
      "metadata": {
        "id": "NT-Jy2ClDlrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Conclusion : According to the aforementioned analysis, a model's accuracy is closely related to the volume of training data and the underlying architecture, especially when the model is trained directly from its own data. On the other hand, if a pretrained model is used, the accuracy depends on the particular test set that is being assessed. It's important to keep in mind that some sample sets could be more difficult to work with than others, and strong results on one set might not translate to all other sets.\n"
      ],
      "metadata": {
        "id": "X6S1I2YSDpRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrach Model : The profound impact on test accuracy stems from both the size of the training dataset and the chosen model architecture. Notably, integrating contemporary architectural features like residual connections, batch normalization, and depthwise separable convolutions into a basic model, alongside employing data augmentation and dropout techniques, significantly boosted test accuracy. Furthermore, expanding the training dataset from 1,000 to 3,000 samples led to substantial improvements in accuracy. Further increasing the dataset to 6,000 samples resulted in test accuracy levels similar to pretrained models. This phenomenon highlights overfitting, where a lack of samples limits the model's ability to generalize. Increasing the dataset size broadens the model's exposure to the underlying data distribution, enhancing test accuracy."
      ],
      "metadata": {
        "id": "FSgP8YsgDrBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained network : However, even while test accuracy is highest when using a pretrained network, the size of the training sample has little bearing on the results. This is a result of the accuracy-measuring model not being trained using the data it handles. Furthermore, methods like data augmentation and fine-tuning do not significantly improve accuracy because the original pretrained model already has high-performance accuracy, owing to the large size of the dataset used in the pretrained VGG16 model (more than 500 MB in size and more than 138 million parameters)."
      ],
      "metadata": {
        "id": "2KR31slTDurP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendations: With a rise in training data from 1,000 to 3,000 samples, the simple scratch model's accuracy increased significantly, from 0.696 to 0.88.4. A test accuracy of about 83.2% was obtained using 3,000 training samples, three contemporary architectures, data augmentation, and dropout approaches. Moreover, the test accuracy was 88% when 5,000 samples were used to train the model. Because Model 10 is the most accurate of the scratch models, it is advised to use it first."
      ],
      "metadata": {
        "id": "UmZPpuV3Dx5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we achieved 100% accuracy with the pre-tuned model, I also think that using pretrained models isn't always the best option. Context-independent reasoning states that the background or context of an image does not influence the ability to discriminate between a dog and a cat in the images used for this exercise. Therefore, as long as the sample photos belong to the same category as the target used to train the model, a pretrained model that can identify images can be applied to any image differentiation job."
      ],
      "metadata": {
        "id": "ZMs4_3ayD0YC"
      }
    }
  ]
}